[34m[1mwandb[0m: Detected [anthropic, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-12-10 15:27:31,158 - LLM_configuration.llm_manager - INFO - wandb initialized successfully
C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\pygame\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_stream, resource_exists
pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)
Hello from the pygame community. https://www.pygame.org/contribute.html
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

==================================================
‚ñ∂ STARTING RUN 1 OF 10
==================================================
2025-12-10 15:27:31,248 - INFO - LLM evaluation started for 3x3 board
2025-12-10 15:27:31,249 - INFO - === GAME 1 ‚Äî GUI MODE ‚Äî openai ===
2025-12-10 15:27:31,840 - INFO - LLM provider set to: openai (model: gpt-5)
2025-12-10 15:27:31,840 - WARNING - Could not generate solver path for evaluation
2025-12-10 15:27:31,841 - INFO - LLM evaluation started for 3x3 board
2025-12-10 15:27:31,841 - INFO - LLM provider set to: openai (model: gpt-5)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f3af' in position 33: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 408, in solve
    logger.info(f"üéØ Generating move {move_number} with {self.provider}")
Message: 'üéØ Generating move 2 with openai'
Arguments: ()
2025-12-10 15:27:31,842 - INFO - üéØ Generating move 2 with openai
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f4cb' in position 33: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 413, in solve
    logger.info(f"üìã EXPERT PROMPT for Move {move_number}:")
Message: 'üìã EXPERT PROMPT for Move 2:'
Arguments: ()
2025-12-10 15:27:31,861 - INFO - üìã EXPERT PROMPT for Move 2:
2025-12-10 15:27:31,866 - INFO - ============================================================
2025-12-10 15:27:31,867 - INFO - You are solving a ZIP PUZZLE. This is a path-finding puzzle where you must visit every cell exactly once.

=== PUZZLE RULES ===
1. Fill ALL 9 cells in one continuous path
2. Start at clue number 1
3. Visit ALL clue numbers in ascending order: 1 -> 2 -> 3 -> ... -> highest
4. End your path at the highest numbered clue
5. Move only horizontally or vertically (NO diagonal moves)
6. Never revisit a cell you've already been to
7. Fill empty cells between clues as needed

=== BOARD INFORMATION ===
Board Size: 3x3
Coordinate System: Each position is (row, column) starting from (0,0)

Coordinate Grid:
(0,0)  (0,1)  (0,2)
(1,0)  (1,1)  (1,2)
(2,0)  (2,1)  (2,2)

=== CLUE LOCATIONS ===
Clue 1: position (1,1)
Clue 2: position (1,2)
Clue 3: position (2,0)

=== CURRENT GAME STATE ===
Progress: 1/9 cells filled
Current Position: (1,1)
Path Taken: (1,1)

Visual Board State:
  .    .    .
  .  [ 1]   3
  4    .    2
Legend: [1],[2],etc = your path order | 1,2,etc = clue numbers | . = empty cell

=== YOUR TASK ===
Available Next Moves: (0,1), (2,1), (1,0), (1,2)

THINK STEP BY STEP AND EXPLAIN YOUR REASONING:

1. ANALYSIS: Where am I now and what's my current situation?

2. STRATEGY: What clue number should I visit next? Where is it located?

3. EVALUATION: For each available move, what are the pros and cons?

4. DECISION: Which move is best and why?

Provide your reasoning in the following format:

THINKING:
[Your detailed analysis here]

MOVE: (row,col)

Example response:
THINKING:
I am currently at position (1,2) and have filled 3 out of 9 cells. Looking at the clues, I need to visit clue 2 next, which is at position (2,1). From my current position, I can move to (1,1), (1,3), (0,2), or (2,2). The move to (1,1) gets me closer to clue 2 and doesn't block any future paths. Moving to (1,3) would take me away from clue 2. Therefore, (1,1) is the optimal choice.

MOVE: (1,1)

Your turn:
2025-12-10 15:27:31,868 - INFO - ============================================================
2025-12-10 15:27:32,693 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:32,697 - ERROR - API call failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 422, in solve
    response_text = self._call_llm_api(prompt)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 316, in _call_llm_api
    return self._call_openai_api(prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 272, in _call_openai_api
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1189, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\u274c' in position 34: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 462, in solve
    logger.error(f"‚ùå Attempt {attempt + 1} failed: {e}")
Message: '‚ùå Attempt 1 failed: Error code: 400 - {\'error\': {\'message\': "Unsupported value: \'temperature\' does not support 0.1 with this model. Only the default (1) value is supported.", \'type\': \'invalid_request_error\', \'param\': \'temperature\', \'code\': \'unsupported_value\'}}'
Arguments: ()
2025-12-10 15:27:32,698 - ERROR - ‚ùå Attempt 1 failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-12-10 15:27:33,092 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:33,095 - ERROR - API call failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 422, in solve
    response_text = self._call_llm_api(prompt)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 316, in _call_llm_api
    return self._call_openai_api(prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 272, in _call_openai_api
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1189, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\u274c' in position 34: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 462, in solve
    logger.error(f"‚ùå Attempt {attempt + 1} failed: {e}")
Message: '‚ùå Attempt 2 failed: Error code: 400 - {\'error\': {\'message\': "Unsupported value: \'temperature\' does not support 0.1 with this model. Only the default (1) value is supported.", \'type\': \'invalid_request_error\', \'param\': \'temperature\', \'code\': \'unsupported_value\'}}'
Arguments: ()
2025-12-10 15:27:33,096 - ERROR - ‚ùå Attempt 2 failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-12-10 15:27:33,117 - WARNING - No move from LLM (stuck: 1/3)
2025-12-10 15:27:33,118 - INFO - Move 1: (-1, -1) - Valid: False, Correct: False, Latency: 1276ms
2025-12-10 15:27:34,119 - INFO - LLM provider set to: openai (model: gpt-5)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f3af' in position 33: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 408, in solve
    logger.info(f"üéØ Generating move {move_number} with {self.provider}")
Message: 'üéØ Generating move 2 with openai'
Arguments: ()
2025-12-10 15:27:34,119 - INFO - üéØ Generating move 2 with openai
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f4cb' in position 33: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 413, in solve
    logger.info(f"üìã EXPERT PROMPT for Move {move_number}:")
Message: 'üìã EXPERT PROMPT for Move 2:'
Arguments: ()
2025-12-10 15:27:34,132 - INFO - üìã EXPERT PROMPT for Move 2:
2025-12-10 15:27:34,139 - INFO - ============================================================
2025-12-10 15:27:34,139 - INFO - You are solving a ZIP PUZZLE. This is a path-finding puzzle where you must visit every cell exactly once.

=== PUZZLE RULES ===
1. Fill ALL 9 cells in one continuous path
2. Start at clue number 1
3. Visit ALL clue numbers in ascending order: 1 -> 2 -> 3 -> ... -> highest
4. End your path at the highest numbered clue
5. Move only horizontally or vertically (NO diagonal moves)
6. Never revisit a cell you've already been to
7. Fill empty cells between clues as needed

=== BOARD INFORMATION ===
Board Size: 3x3
Coordinate System: Each position is (row, column) starting from (0,0)

Coordinate Grid:
(0,0)  (0,1)  (0,2)
(1,0)  (1,1)  (1,2)
(2,0)  (2,1)  (2,2)

=== CLUE LOCATIONS ===
Clue 1: position (1,1)
Clue 2: position (1,2)
Clue 3: position (2,0)

=== CURRENT GAME STATE ===
Progress: 1/9 cells filled
Current Position: (1,1)
Path Taken: (1,1)

Visual Board State:
  .    .    .
  .  [ 1]   3
  4    .    2
Legend: [1],[2],etc = your path order | 1,2,etc = clue numbers | . = empty cell

=== YOUR TASK ===
Available Next Moves: (0,1), (2,1), (1,0), (1,2)

THINK STEP BY STEP AND EXPLAIN YOUR REASONING:

1. ANALYSIS: Where am I now and what's my current situation?

2. STRATEGY: What clue number should I visit next? Where is it located?

3. EVALUATION: For each available move, what are the pros and cons?

4. DECISION: Which move is best and why?

Provide your reasoning in the following format:

THINKING:
[Your detailed analysis here]

MOVE: (row,col)

Example response:
THINKING:
I am currently at position (1,2) and have filled 3 out of 9 cells. Looking at the clues, I need to visit clue 2 next, which is at position (2,1). From my current position, I can move to (1,1), (1,3), (0,2), or (2,2). The move to (1,1) gets me closer to clue 2 and doesn't block any future paths. Moving to (1,3) would take me away from clue 2. Therefore, (1,1) is the optimal choice.

MOVE: (1,1)

Your turn:
2025-12-10 15:27:34,140 - INFO - ============================================================
2025-12-10 15:27:34,527 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:34,530 - ERROR - API call failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 422, in solve
    response_text = self._call_llm_api(prompt)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 316, in _call_llm_api
    return self._call_openai_api(prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 272, in _call_openai_api
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1189, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\thame\miniconda3\envs\zip\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\thame\miniconda3\envs\zip\Lib\logging\__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "C:\Users\thame\miniconda3\envs\zip\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\u274c' in position 34: character maps to <undefined>
Call stack:
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1032, in _bootstrap
    self._bootstrap_inner()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "C:\Users\thame\miniconda3\envs\zip\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\zip_llm_tests.py", line 177, in <lambda>
    target=lambda: game.solve_with_llm(provider),
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\UI\GUI.py", line 712, in solve_with_llm
    result = llm_solver.solve(self.board, self.path, next_number)
  File "C:\Users\thame\Desktop\CS5900\zip\Zip_Solver-with_LLM\src\LLM_configuration\llm_manager.py", line 462, in solve
    logger.error(f"‚ùå Attempt {attempt + 1} failed: {e}")
Message: '‚ùå Attempt 1 failed: Error code: 400 - {\'error\': {\'message\': "Unsupported value: \'temperature\' does not support 0.1 with this model. Only the default (1) value is supported.", \'type\': \'invalid_request_error\', \'param\': \'temperature\', \'code\': \'unsupported_value\'}}'
Arguments: ()
2025-12-10 15:27:34,533 - ERROR - ‚ùå Attempt 1 failed: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
